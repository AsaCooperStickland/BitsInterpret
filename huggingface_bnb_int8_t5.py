# -*- coding: utf-8 -*-
"""HuggingFace_bnb_int8_T5

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x-cv_-XXCCYeDyUOO9xVAvsa0CD-8ZVa

# HuggingFace meets `bitsandbytes` for lighter models on GPU for inference

## Running T5-11b on Google Colab

<center>
 <img src="https://s3.amazonaws.com/moonup/production/uploads/1659861207959-62441d1d9fdefb55a0b7d12c.png">
 </center>

You can run your own 8-bit model on any HuggingFace ðŸ¤— model with just few lines of code. This notebook shows how to do it with a `T5` model that would usually require 12GB of GPU RAM.
Install the dependencies below first!
"""
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import torch
"""Implementation of different utility functions for adapter layers."""
from dataclasses import dataclass
import torch.nn as nn
import torch.nn.functional as F

from bitsandbytes.nn import Linear8bitLt

class LoraLinear8bit(Linear8bitLt):
    def __init__(
        self,
        weight,
        bias=None,
        adapter_dim=0,
        parallel=False,
        batchensemble=False,
        adapter_bias=False,
        up_scale=128,
        # init_svd=False,
    ):
        self.out_features, self.in_features = weight.shape
        bias_used = bias is None
        super().__init__(self.in_features, self.out_features, bias=bias_used)

        self.register_buffer("weight", weight.requires_grad_(False))
        self.bias = bias
        self.parallel = parallel
        # self.down_scale = down_scale
        self.up_scale = up_scale
        # self.init_svd = init_svd

        if adapter_dim > 0:
            self.adapter_down = nn.Linear(self.in_features, adapter_dim, bias=False)
            self.adapter_up = nn.Linear(adapter_dim, self.out_features, bias=False)

            self.adapter_dim = adapter_dim
            if adapter_bias:
                self.adapter_bias = nn.Parameter(torch.zeros(self.out_features))
            else:
                self.adapter_bias = None

            nn.init.zeros_(self.adapter_up.weight)
        else:
            self.adapter_down = None

        if batchensemble:
            self.adapter_before = nn.Parameter(torch.zeros(self.in_features))
            self.adapter_after = nn.Parameter(torch.zeros(self.out_features))
            if adapter_bias:
                self.adapter_bias2 = nn.Parameter(torch.zeros(self.out_features))
            else:
                self.adapter_bias2 = None
        else:
            self.adapter_before = None
            self.adapter_after = None
            self.adapter_bias2 = None

    def forward(self, input):
        if self.adapter_before is not None:
            input = input * (1.0 + 128.0**0.5 * self.adapter_before)
        out = super().forward(input)
        if self.adapter_after is not None:
            out = out * (1.0 + 128.0**0.5 * self.adapter_after)
        if self.adapter_bias2 is not None:
            out = out + self.adapter_bias2

        if self.parallel and self.adapter_down is not None:
            input = self.adapter_down(input)
            # input = (self.down_scale / self.in_features) * self.adapter_down(input)
            out = out + (self.up_scale / self.adapter_dim) * self.adapter_up(input)
            if self.adapter_bias is not None:
                out = out + self.adapter_bias
            # out = out + self.adapter_bias + (128.0 / self.adapter_dim) * self.adapter(input)
        if not self.parallel and self.adapter_down is not None:
            out = (
                out + self.adapter_bias + (128.0 / self.adapter_dim) * self.adapter(out)
            )
            if self.adapter_bias is not None:
                out = out + self.adapter_bias

        return out

    @classmethod
    def from_linear(cls, linear: Linear8bitLt, **kwargs) -> "LoraLinear8bit":
        return cls(linear.weight, linear.bias, **kwargs)

    def __repr__(self):
        return f"{self.__class__.__name__}({self.in_features}, {self.out_features})"


def lorafy_(model, config):
    for module in list(model.modules()):
        for name, child in module.named_children():
            if isinstance(child, Linear8bitLt) and name in config.layer_list.split(","):
                print(name, child)
                setattr(
                    module,
                    name,
                    LoraLinear8bit.from_linear(
                        child,
                        adapter_dim=config.adapter_size,
                        parallel=config.parallel,
                        batchensemble=config.batchensemble,
                        adapter_bias=config.lora_bias,
                        up_scale=config.up_scale,
                        # init_svd=config.init_svd,
                    ),
                )

@dataclass
class Config:
    adapter_size: int = 4
    parallel: bool = True 
    batchensemble: bool = False   
    lora_bias: bool = False
    # down_scale: int = 728
    up_scale: int = 128
    # init_svd: bool = False
    layer_list: str = "wo"
    
    
    

config = Config()

"""## Chose your model

Rerun this cell if you want to change the model!
"""

model_name = "t5-small" #@param ["t5-small", "t5-11b-sharded", "t5-3b-sharded"]

# T5-3b and T5-11B are supported!
# We need sharded weights otherwise we get CPU OOM errors
model_id=f"ybelkada/{model_name}"
if "sharded" in model_name:
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model_8bit = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map="auto", load_in_8bit=True)
else:
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model_8bit = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map="auto", load_in_8bit=True)

"""## Use 8bit models with `t5-3b-sharded` ðŸ¤—

Let's check the memory footprint of this model! ðŸª¶
"""

print(model_8bit.get_memory_footprint())

"""For `t5-3b` the int8 model is about ~2.9GB! whereas the original model has 11GB. For `t5-11b` the int8 model is about ~11GB vs 42GB for the original model.
Now let's generate and see the qualitative results of the 8bit model!
"""

def test_generate(model):
    max_new_tokens = 50
    
    input_ids = tokenizer(
        "translate English to German: Hello my name is Younes and I am a Machine Learning Engineer at Hugging Face", return_tensors="pt"
    ).input_ids  
    
    outputs = model.generate(input_ids, max_new_tokens=max_new_tokens)
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))

test_generate(model_8bit)
lorafy_(model_8bit, config)
test_generate(model_8bit)

